{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.7, max_features=None, min_df=3,\n",
       "                ngram_range=(1, 1), preprocessor=None,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}),\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, max_df=0.7, min_df=3)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26747"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 26747)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LDA(X_train, Number_Of_Topics, alpha, beta, n_iter=10):\n",
    "    n_kw = np.zeros((Number_Of_Topics, X_train.shape[1]))\n",
    "    n_dk = np.zeros((X_train.shape[0], Number_Of_Topics))\n",
    "    n_k = np.zeros(Number_Of_Topics)\n",
    "    docs, words = X_train.nonzero()\n",
    "    z = np.random.choice(Number_Of_Topics, len(docs))\n",
    "    \n",
    "    for doc, word, cur_z in zip(docs, words, z):\n",
    "        n_dk[doc, cur_z] += 1\n",
    "        n_kw[cur_z, word] += 1\n",
    "        n_k[cur_z] += 1\n",
    "    \n",
    "    for cur_iter in range(n_iter):\n",
    "        for i in range(len(docs)):\n",
    "            cur_word = words[i]\n",
    "            cur_doc = docs[i]\n",
    "            cur_topic = z[i]\n",
    "            \n",
    "            n_dk[cur_doc, cur_topic] -= 1\n",
    "            n_kw[cur_topic, cur_word] -= 1\n",
    "            n_k[cur_topic] -= 1\n",
    "            \n",
    "            p = (n_dk[cur_doc, :] + alpha) * (n_kw[:, cur_word] + beta[cur_word]) / (n_k + beta.sum())\n",
    "            z[i] = np.random.choice(np.arange(Number_Of_Topics), p=p / p.sum())\n",
    "            \n",
    "            n_dk[cur_doc, z[i]] += 1\n",
    "            n_kw[z[i], cur_word] += 1\n",
    "            n_k[z[i]] += 1\n",
    "    \n",
    "    return z, n_kw, n_dk, n_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_Of_Topics=20\n",
    "z, n_kw, n_dk, n_k = LDA(X_train, Number_Of_Topics, 1 * np.ones(Number_Of_Topics),1 * np.ones(X_train.shape[1]), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "на таком словаре работало очень долго, так что пришлось запустить всего 30 итераций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\tdoes\tjust\tknow\tlike\tneed\tproblem\tthanks\tuse\tusing\twindows\n",
      "Topic 1:\tcalled\tedu\thear\tposted\trecall\tsaw\tsend\tsteve\ttimes\twondering\n",
      "Topic 2:\tinterested\tmissing\tnegative\tnoticed\tpast\tschool\tsort\ttank\tthank\twouldn\n",
      "Topic 3:\tchildren\tgovernment\tisrael\tisraeli\tjewish\tjews\tkilled\tsaid\twar\tworld\n",
      "Topic 4:\tcheers\tinstead\tpair\tposting\tproblem\tputting\ttest\ttheory\twanted\tword\n",
      "Topic 5:\t14\tend\tma\tmi\tmn\tmq\tmr\tmt\tmw\tpl\n",
      "Topic 6:\tbanks\tedu\tgeb\tgordon\tintellect\tpitt\tshameful\tskepticism\tsoon\tsurrender\n",
      "Topic 7:\tair\tbike\tcar\tcars\tengine\tground\thigh\tlight\tmiles\troad\n",
      "Topic 8:\tgame\tgames\thockey\tleague\tplay\tplayers\tseason\tteam\twin\tyear\n",
      "Topic 9:\t205\tal\tcom\tdave\tintergraph\tinternet\tlook\toffice\tuucp\tuunet\n",
      "Topic 10:\tbike\tcome\tcomments\tedu\tgreat\tmichael\tobvious\tposting\tprobably\tvarious\n",
      "Topic 11:\tdon\tgood\tjust\tknow\tlike\tmake\tpeople\treally\tthink\ttime\n",
      "Topic 12:\tarticle\tedu\tfrank\tgood\tguy\thand\thuh\tmiddle\tsmall\tthought\n",
      "Topic 13:\t1993\tavailable\tdata\tedu\tfollowing\tgeneral\tinformation\tprogram\tspace\tuniversity\n",
      "Topic 14:\tarticle\tbrad\tdoing\telectrical\tharvard\theard\tjust\tmind\tmodel\tremember\n",
      "Topic 15:\tbelieve\tbible\tchristian\tchristians\tdoes\tgod\tjesus\tpeople\treligion\tsay\n",
      "Topic 16:\tblew\tbob\tbobbe\tcom\tico\tqueens\tsank\tsea\tstay\ttek\n",
      "Topic 17:\tencryption\tgovernment\tgun\tkey\tlaw\tlegal\tpeople\tpublic\tuse\tused\n",
      "Topic 18:\tbike\tbikes\tcar\tdod\tmotorcycle\tmotorcycles\trear\tride\tstudent\tvision\n",
      "Topic 19:\t10\t11\t12\t13\t14\t15\t17\t20\t24\t30\n"
     ]
    }
   ],
   "source": [
    "top_words = np.argsort(n_kw, axis=1)[:, :-11:-1]\n",
    "\n",
    "for topic in range(20):\n",
    "    doc = np.zeros((1, X_train.shape[1]))\n",
    "    for word in top_words[topic]:\n",
    "        doc[0, word] = 1\n",
    "    print('Topic {}:\\t{}'.format(topic, '\\t'.join(vectorizer.inverse_transform(doc)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "темы очень плохо выявляются, хотя какие-то выделить можно, например \n",
    "15 вера\n",
    "17 политика\n",
    "18 транспорт\n",
    "5 что-то связано с windows\n",
    "7 машины\n",
    "соотнести конкретно с темами изначальными можно с трудом\n",
    "поэтому уменьшим словарь и запустим большее число итераций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.04, max_features=None, min_df=11,\n",
       "                ngram_range=(1, 1), preprocessor=None,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}),\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\n",
    "                             analyzer='word', binary=True, max_df=0.04, min_df=11)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9544"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, слов втрое меньше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 9544)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\t100\t50\tasking\tcondition\toffer\toriginal\tprice\tsale\tsell\tshipping\n",
      "Topic 1:\tapplication\tcode\tfile\tfiles\tftp\tgraphics\trunning\tserver\tversion\twindow\n",
      "Topic 2:\tboard\tcard\tcomputer\tdisk\tmac\tmemory\tmonitor\tpc\tspeed\tvideo\n",
      "Topic 3:\tcertain\tdeal\texactly\tknown\tmind\tnice\tposting\tsimilar\tstuff\ttried\n",
      "Topic 4:\tadvance\tanybody\tappreciate\tappreciated\thi\tinfo\tnet\treply\tthank\twondering\n",
      "Topic 5:\tarticle\thear\tinteresting\tnews\treading\trecall\tsounds\tstuff\tuniversity\twonder\n",
      "Topic 6:\t1993\tcenter\tdate\tearth\tnasa\tresearch\tscience\tspace\tsystems\tuniversity\n",
      "Topic 7:\t11\t12\t13\t14\t16\t17\t18\t23\t24\t25\n",
      "Topic 8:\tbible\tchrist\tchristian\tchristians\tchurch\tfaith\tjesus\tjohn\tlove\tman\n",
      "Topic 9:\tbanks\tcause\tdisease\tfood\tgordon\tpitt\tskepticism\tsoon\tsurrender\tusually\n",
      "Topic 10:\tchip\tclipper\tencryption\tkey\tkeys\tmessage\tphone\tpublic\tsecure\tsecurity\n",
      "Topic 11:\tamerican\tbusiness\tcare\tchange\tclinton\thouse\tmoney\tpay\tpresident\tstates\n",
      "Topic 12:\tcontrol\tcrime\tgun\tguns\tlaw\tlaws\tpolice\trights\tself\tweapons\n",
      "Topic 13:\tcurrent\tdifference\theat\thot\tinteresting\tok\treading\tsort\tstuff\twater\n",
      "Topic 14:\tgame\tgames\thockey\tleague\tplay\tplayers\tseason\tteam\tteams\twin\n",
      "Topic 15:\tagree\targument\tcertainly\tclaim\tdiscussion\tevidence\tsaying\tsense\tsimply\tstatement\n",
      "Topic 16:\tbike\tcar\tcars\tengine\tleft\tlight\tmiles\troad\tturn\twent\n",
      "Topic 17:\tchildren\tcountry\tisrael\tisraeli\tjewish\tjews\tkilled\tmilitary\twar\twomen\n",
      "Topic 18:\tbtw\tdeleted\tguess\they\tlooks\toh\tok\tsomebody\tsorry\tyeah\n",
      "Topic 19:\tal\tbob\tdave\tinternet\tmentioned\tphone\tsorry\tstay\tuucp\tvice\n"
     ]
    }
   ],
   "source": [
    "Number_Of_Topics=20\n",
    "z, n_kw, n_dk, n_k = LDA(X_train, Number_Of_Topics, 1 * np.ones(Number_Of_Topics),1 * np.ones(X_train.shape[1]), 60)\n",
    "top_words = np.argsort(n_kw, axis=1)[:, :-11:-1]\n",
    "\n",
    "for topic in range(20):\n",
    "    doc = np.zeros((1, X_train.shape[1]))\n",
    "    for word in top_words[topic]:\n",
    "        doc[0, word] = 1\n",
    "    print('Topic {}:\\t{}'.format(topic, '\\t'.join(vectorizer.inverse_transform(doc)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "все намного лучше\n",
    "здесь темы выявяются гораздо, а некоторые очень хорошо соотносятся с изначальными: 14 хоккей, 8 вера и так далее"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
